{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport random\nimport pickle\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:21:51.733134Z","iopub.execute_input":"2025-04-29T13:21:51.733375Z","iopub.status.idle":"2025-04-29T13:21:51.741232Z","shell.execute_reply.started":"2025-04-29T13:21:51.733356Z","shell.execute_reply":"2025-04-29T13:21:51.740418Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport random\nimport pickle\nimport os\n\nclass TicTacToe:\n    def __init__(self):\n        self.board = np.zeros((3, 3), dtype=int)\n        # 1 for X (player), -1 for O (agent)\n        self.player_symbol = 1\n        self.agent_symbol = -1\n        self.reset()\n        \n    def reset(self):\n        self.board = np.zeros((3, 3), dtype=int)\n        self.current_state = self.get_state()\n        self.game_over = False\n        return self.current_state\n    \n    def get_state(self):\n        return str(self.board.reshape(9))\n    \n    def get_valid_moves(self):\n        valid_moves = []\n        for i in range(3):\n            for j in range(3):\n                if self.board[i, j] == 0:\n                    valid_moves.append((i, j))\n        return valid_moves\n    \n    def make_move(self, position, symbol):\n        if self.board[position] != 0 or self.game_over:\n            return False\n        \n        self.board[position] = symbol\n        self.current_state = self.get_state()\n        \n        if self.check_win(symbol):\n            self.game_over = True\n        elif len(self.get_valid_moves()) == 0:\n            self.game_over = True\n            \n        return True\n    \n    def check_win(self, symbol):\n        for i in range(3):\n            if np.all(self.board[i, :] == symbol):\n                return True\n        \n        for i in range(3):\n            if np.all(self.board[:, i] == symbol):\n                return True\n        \n        if np.all(np.diag(self.board) == symbol):\n            return True\n        if np.all(np.diag(np.fliplr(self.board)) == symbol):\n            return True\n        \n        return False\n    \n    def get_reward(self, symbol):\n        if self.check_win(symbol):\n            return 1\n        elif self.check_win(-symbol):\n            return -1\n        elif len(self.get_valid_moves()) == 0:\n            return 0.5  # Draw\n        else:\n            return 0  # Game not over yet\n    \n    def print_board(self):\n        symbols = {0: ' ', 1: 'X', -1: 'O'}\n        for i in range(3):\n            row = []\n            for j in range(3):\n                row.append(symbols[self.board[i, j]])\n            print(' | '.join(row))\n            if i < 2:\n                print('---------')\n        print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:24:11.264758Z","iopub.execute_input":"2025-04-29T13:24:11.265055Z","iopub.status.idle":"2025-04-29T13:24:11.278865Z","shell.execute_reply.started":"2025-04-29T13:24:11.265032Z","shell.execute_reply":"2025-04-29T13:24:11.277862Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class QLearningAgent:\n    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.9):\n        # epsilon: exploration rate\n        # alpha: learning rate\n        # gamma: discount factor\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.gamma = gamma\n        self.q_table = {}\n    \n    def get_q_value(self, state, action):\n        if state not in self.q_table:\n            self.q_table[state] = {}\n        if action not in self.q_table[state]:\n            self.q_table[state][action] = 0.0\n        return self.q_table[state][action]\n    \n    def choose_action(self, state, valid_moves, is_training=True):\n        if not valid_moves:\n            return None\n        \n        # Exploration vs. exploitation\n        if is_training and random.random() < self.epsilon:\n            return random.choice(valid_moves)\n        else:\n            q_values = [self.get_q_value(state, action) for action in valid_moves]\n            max_q = max(q_values)\n            \n            best_actions = [valid_moves[i] for i, q in enumerate(q_values) if q == max_q]\n            return random.choice(best_actions)\n    \n    def update_q_value(self, state, action, reward, next_state, next_valid_moves):\n        if not next_valid_moves:\n            max_next_q = 0\n        else:\n            max_next_q = max([self.get_q_value(next_state, next_action) for next_action in next_valid_moves])\n        \n        current_q = self.get_q_value(state, action)\n        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n        \n        if state not in self.q_table:\n            self.q_table[state] = {}\n        self.q_table[state][action] = new_q\n    \n    def save_model(self, filename='tictactoe_q_model.pkl'):\n        #Save the Q-table to a file\n        with open(filename, 'wb') as f:\n            pickle.dump(self.q_table, f)\n        print(f\"Model saved to {filename}\")\n    \n    def load_model(self, filename='tictactoe_q_model.pkl'):\n        #Load the Q-table from a file\n        if os.path.exists(filename):\n            with open(filename, 'rb') as f:\n                self.q_table = pickle.load(f)\n            print(f\"Model loaded from {filename}\")\n            return True\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:24:24.241760Z","iopub.execute_input":"2025-04-29T13:24:24.242529Z","iopub.status.idle":"2025-04-29T13:24:24.252720Z","shell.execute_reply.started":"2025-04-29T13:24:24.242499Z","shell.execute_reply":"2025-04-29T13:24:24.251961Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def train_agent(episodes=10000):\n    env = TicTacToe()\n    agent = QLearningAgent()\n    \n    print(f\"Training for {episodes} episodes...\")\n    \n    win_count = 0\n    loss_count = 0\n    draw_count = 0\n    \n    for episode in range(episodes):\n        state = env.reset()\n        done = False\n        \n        player_turn = random.choice([True, False])\n        \n        while not done:\n            if player_turn:\n                valid_moves = env.get_valid_moves()\n                if valid_moves:\n                    random_move = random.choice(valid_moves)\n                    env.make_move(random_move, env.player_symbol)\n                player_turn = False\n            else:\n                valid_moves = env.get_valid_moves()\n                if not valid_moves:\n                    done = True\n                    continue\n                \n                action = agent.choose_action(state, valid_moves)\n                \n                old_state = state\n                \n                env.make_move(action, env.agent_symbol)\n                \n                new_state = env.get_state()\n                \n                reward = env.get_reward(env.agent_symbol)\n                \n                next_valid_moves = env.get_valid_moves()\n                \n                agent.update_q_value(old_state, action, reward, new_state, next_valid_moves)\n                \n                state = new_state\n                player_turn = True\n                \n                if env.game_over:\n                    done = True\n        \n        if env.check_win(env.agent_symbol):\n            win_count += 1\n        elif env.check_win(env.player_symbol):\n            loss_count += 1\n        else:\n            draw_count += 1\n        \n        if (episode + 1) % 1000 == 0:\n            print(f\"Episode {episode + 1}/{episodes}\")\n            print(f\"Wins: {win_count}, Losses: {loss_count}, Draws: {draw_count}\")\n            win_rate = win_count / 1000\n            loss_rate = loss_count / 1000\n            draw_rate = draw_count / 1000\n            print(f\"Win rate: {win_rate:.2f}, Loss rate: {loss_rate:.2f}, Draw rate: {draw_rate:.2f}\")\n            \n            # Reset counts\n            win_count = 0\n            loss_count = 0\n            draw_count = 0\n    \n    agent.save_model()\n    return agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:24:40.930508Z","iopub.execute_input":"2025-04-29T13:24:40.930784Z","iopub.status.idle":"2025-04-29T13:24:40.940455Z","shell.execute_reply.started":"2025-04-29T13:24:40.930763Z","shell.execute_reply":"2025-04-29T13:24:40.939531Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"agent = train_agent(episodes=50000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:25:04.676213Z","iopub.execute_input":"2025-04-29T13:25:04.676547Z","iopub.status.idle":"2025-04-29T13:26:04.508796Z","shell.execute_reply.started":"2025-04-29T13:25:04.676524Z","shell.execute_reply":"2025-04-29T13:26:04.507909Z"}},"outputs":[{"name":"stdout","text":"Training for 50000 episodes...\nEpisode 1000/50000\nWins: 561, Losses: 364, Draws: 75\nWin rate: 0.56, Loss rate: 0.36, Draw rate: 0.07\nEpisode 2000/50000\nWins: 654, Losses: 263, Draws: 83\nWin rate: 0.65, Loss rate: 0.26, Draw rate: 0.08\nEpisode 3000/50000\nWins: 666, Losses: 246, Draws: 88\nWin rate: 0.67, Loss rate: 0.25, Draw rate: 0.09\nEpisode 4000/50000\nWins: 684, Losses: 252, Draws: 64\nWin rate: 0.68, Loss rate: 0.25, Draw rate: 0.06\nEpisode 5000/50000\nWins: 708, Losses: 222, Draws: 70\nWin rate: 0.71, Loss rate: 0.22, Draw rate: 0.07\nEpisode 6000/50000\nWins: 735, Losses: 213, Draws: 52\nWin rate: 0.73, Loss rate: 0.21, Draw rate: 0.05\nEpisode 7000/50000\nWins: 719, Losses: 209, Draws: 72\nWin rate: 0.72, Loss rate: 0.21, Draw rate: 0.07\nEpisode 8000/50000\nWins: 701, Losses: 241, Draws: 58\nWin rate: 0.70, Loss rate: 0.24, Draw rate: 0.06\nEpisode 9000/50000\nWins: 720, Losses: 221, Draws: 59\nWin rate: 0.72, Loss rate: 0.22, Draw rate: 0.06\nEpisode 10000/50000\nWins: 751, Losses: 179, Draws: 70\nWin rate: 0.75, Loss rate: 0.18, Draw rate: 0.07\nEpisode 11000/50000\nWins: 723, Losses: 210, Draws: 67\nWin rate: 0.72, Loss rate: 0.21, Draw rate: 0.07\nEpisode 12000/50000\nWins: 729, Losses: 213, Draws: 58\nWin rate: 0.73, Loss rate: 0.21, Draw rate: 0.06\nEpisode 13000/50000\nWins: 733, Losses: 213, Draws: 54\nWin rate: 0.73, Loss rate: 0.21, Draw rate: 0.05\nEpisode 14000/50000\nWins: 743, Losses: 195, Draws: 62\nWin rate: 0.74, Loss rate: 0.20, Draw rate: 0.06\nEpisode 15000/50000\nWins: 726, Losses: 206, Draws: 68\nWin rate: 0.73, Loss rate: 0.21, Draw rate: 0.07\nEpisode 16000/50000\nWins: 732, Losses: 206, Draws: 62\nWin rate: 0.73, Loss rate: 0.21, Draw rate: 0.06\nEpisode 17000/50000\nWins: 719, Losses: 213, Draws: 68\nWin rate: 0.72, Loss rate: 0.21, Draw rate: 0.07\nEpisode 18000/50000\nWins: 737, Losses: 188, Draws: 75\nWin rate: 0.74, Loss rate: 0.19, Draw rate: 0.07\nEpisode 19000/50000\nWins: 728, Losses: 197, Draws: 75\nWin rate: 0.73, Loss rate: 0.20, Draw rate: 0.07\nEpisode 20000/50000\nWins: 740, Losses: 191, Draws: 69\nWin rate: 0.74, Loss rate: 0.19, Draw rate: 0.07\nEpisode 21000/50000\nWins: 744, Losses: 173, Draws: 83\nWin rate: 0.74, Loss rate: 0.17, Draw rate: 0.08\nEpisode 22000/50000\nWins: 793, Losses: 145, Draws: 62\nWin rate: 0.79, Loss rate: 0.14, Draw rate: 0.06\nEpisode 23000/50000\nWins: 720, Losses: 216, Draws: 64\nWin rate: 0.72, Loss rate: 0.22, Draw rate: 0.06\nEpisode 24000/50000\nWins: 750, Losses: 174, Draws: 76\nWin rate: 0.75, Loss rate: 0.17, Draw rate: 0.08\nEpisode 25000/50000\nWins: 720, Losses: 196, Draws: 84\nWin rate: 0.72, Loss rate: 0.20, Draw rate: 0.08\nEpisode 26000/50000\nWins: 730, Losses: 197, Draws: 73\nWin rate: 0.73, Loss rate: 0.20, Draw rate: 0.07\nEpisode 27000/50000\nWins: 773, Losses: 170, Draws: 57\nWin rate: 0.77, Loss rate: 0.17, Draw rate: 0.06\nEpisode 28000/50000\nWins: 736, Losses: 172, Draws: 92\nWin rate: 0.74, Loss rate: 0.17, Draw rate: 0.09\nEpisode 29000/50000\nWins: 738, Losses: 176, Draws: 86\nWin rate: 0.74, Loss rate: 0.18, Draw rate: 0.09\nEpisode 30000/50000\nWins: 766, Losses: 171, Draws: 63\nWin rate: 0.77, Loss rate: 0.17, Draw rate: 0.06\nEpisode 31000/50000\nWins: 745, Losses: 183, Draws: 72\nWin rate: 0.74, Loss rate: 0.18, Draw rate: 0.07\nEpisode 32000/50000\nWins: 759, Losses: 169, Draws: 72\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 33000/50000\nWins: 754, Losses: 172, Draws: 74\nWin rate: 0.75, Loss rate: 0.17, Draw rate: 0.07\nEpisode 34000/50000\nWins: 771, Losses: 161, Draws: 68\nWin rate: 0.77, Loss rate: 0.16, Draw rate: 0.07\nEpisode 35000/50000\nWins: 764, Losses: 167, Draws: 69\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 36000/50000\nWins: 753, Losses: 167, Draws: 80\nWin rate: 0.75, Loss rate: 0.17, Draw rate: 0.08\nEpisode 37000/50000\nWins: 739, Losses: 181, Draws: 80\nWin rate: 0.74, Loss rate: 0.18, Draw rate: 0.08\nEpisode 38000/50000\nWins: 780, Losses: 167, Draws: 53\nWin rate: 0.78, Loss rate: 0.17, Draw rate: 0.05\nEpisode 39000/50000\nWins: 735, Losses: 198, Draws: 67\nWin rate: 0.73, Loss rate: 0.20, Draw rate: 0.07\nEpisode 40000/50000\nWins: 741, Losses: 184, Draws: 75\nWin rate: 0.74, Loss rate: 0.18, Draw rate: 0.07\nEpisode 41000/50000\nWins: 753, Losses: 161, Draws: 86\nWin rate: 0.75, Loss rate: 0.16, Draw rate: 0.09\nEpisode 42000/50000\nWins: 759, Losses: 184, Draws: 57\nWin rate: 0.76, Loss rate: 0.18, Draw rate: 0.06\nEpisode 43000/50000\nWins: 760, Losses: 167, Draws: 73\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 44000/50000\nWins: 784, Losses: 151, Draws: 65\nWin rate: 0.78, Loss rate: 0.15, Draw rate: 0.07\nEpisode 45000/50000\nWins: 765, Losses: 159, Draws: 76\nWin rate: 0.77, Loss rate: 0.16, Draw rate: 0.08\nEpisode 46000/50000\nWins: 761, Losses: 172, Draws: 67\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 47000/50000\nWins: 760, Losses: 169, Draws: 71\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 48000/50000\nWins: 761, Losses: 171, Draws: 68\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 49000/50000\nWins: 763, Losses: 165, Draws: 72\nWin rate: 0.76, Loss rate: 0.17, Draw rate: 0.07\nEpisode 50000/50000\nWins: 761, Losses: 162, Draws: 77\nWin rate: 0.76, Loss rate: 0.16, Draw rate: 0.08\nModel saved to tictactoe_q_model.pkl\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def play_game(agent, use_trained_model=True):\n    env = TicTacToe()\n    if use_trained_model:\n        agent.load_model()\n    \n    print(\"New TicTacToe Game!\")\n    print(\"You are 'X', the agent is 'O'\")\n    print(\"Enter your move as row,col (e.g., '0,0' for top-left)\")\n    \n    player_turn = True  \n    \n    done = False\n    state = env.get_state()\n    \n    while not done:\n        if player_turn:\n            env.print_board()\n            valid_moves = env.get_valid_moves()\n            if not valid_moves:\n                print(\"It's a draw!\")\n                done = True\n                continue\n            \n            valid_input = False\n            while not valid_input:\n                try:\n                    move_input = input(\"Your move (row,col): \")\n                    row, col = map(int, move_input.split(','))\n                    move = (row, col)\n                    if move in valid_moves:\n                        valid_input = True\n                    else:\n                        print(\"Invalid move! Try again.\")\n                except ValueError:\n                    print(\"Invalid input! Please enter as 'row,col' (e.g., '0,0').\")\n            \n            env.make_move(move, env.player_symbol)\n            if env.check_win(env.player_symbol):\n                env.print_board()\n                print(\"You win!\")\n                done = True\n            player_turn = False\n        else:\n            valid_moves = env.get_valid_moves()\n            if not valid_moves:\n                env.print_board()\n                print(\"It's a draw!\")\n                done = True\n                continue\n            \n            action = agent.choose_action(state, valid_moves, is_training=False)\n            env.make_move(action, env.agent_symbol)\n            print(f\"Agent chose: {action}\")\n            \n            if env.check_win(env.agent_symbol):\n                env.print_board()\n                print(\"Agent wins!\")\n                done = True\n            \n            state = env.get_state()\n            player_turn = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:26:26.704713Z","iopub.execute_input":"2025-04-29T13:26:26.705006Z","iopub.status.idle":"2025-04-29T13:26:26.713836Z","shell.execute_reply.started":"2025-04-29T13:26:26.704983Z","shell.execute_reply":"2025-04-29T13:26:26.712732Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"while True:\n    play_game(agent)\n    play_again = input(\"Play again? (y/n): \")\n    if play_again.lower() != 'y':\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T13:32:10.917147Z","iopub.execute_input":"2025-04-29T13:32:10.917911Z","iopub.status.idle":"2025-04-29T13:34:38.968154Z","shell.execute_reply.started":"2025-04-29T13:32:10.917881Z","shell.execute_reply":"2025-04-29T13:34:38.967308Z"}},"outputs":[{"name":"stdout","text":"Model loaded from tictactoe_q_model.pkl\nNew TicTacToe Game!\nYou are 'X', the agent is 'O'\nEnter your move as row,col (e.g., '0,0' for top-left)\n  |   |  \n---------\n  |   |  \n---------\n  |   |  \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (row,col):  1,2\n"},{"name":"stdout","text":"Agent chose: (0, 0)\nO |   |  \n---------\n  |   | X\n---------\n  |   |  \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (row,col):  1,1\n"},{"name":"stdout","text":"Agent chose: (2, 0)\nO |   |  \n---------\n  | X | X\n---------\nO |   |  \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (row,col):  1,0\n"},{"name":"stdout","text":"O |   |  \n---------\nX | X | X\n---------\nO |   |  \n\nYou win!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Play again? (y/n):  y\n"},{"name":"stdout","text":"Model loaded from tictactoe_q_model.pkl\nNew TicTacToe Game!\nYou are 'X', the agent is 'O'\nEnter your move as row,col (e.g., '0,0' for top-left)\n  |   |  \n---------\n  |   |  \n---------\n  |   |  \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (row,col):  1,1\n"},{"name":"stdout","text":"Agent chose: (0, 0)\nO |   |  \n---------\n  | X |  \n---------\n  |   |  \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (row,col):  2,2\n"},{"name":"stdout","text":"Agent chose: (1, 0)\nO |   |  \n---------\nO | X |  \n---------\n  |   | X\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Your move (row,col):  0,2\n"},{"name":"stdout","text":"Agent chose: (2, 0)\nO |   | X\n---------\nO | X |  \n---------\nO |   | X\n\nAgent wins!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Play again? (y/n):  n\n"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}